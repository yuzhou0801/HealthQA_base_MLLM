{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a043dac-fc70-43af-942f-c7365efbf9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /root/autodl-tmp/trained_model\n",
      "Tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Health 类别数据量: 818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/818 [00:00<?, ?it/s]/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Testing: 100%|██████████| 818/818 [2:45:13<00:00, 12.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试完成！\n",
      "Health 类别准确率: 50.24%\n",
      "平均推理时间: 12.1154 秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# 设备设置\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载模型\n",
    "def load_model(model_path):\n",
    "    print(f\"Loading model from: {model_path}\")  # 调试信息\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    print(\"Tokenizer loaded successfully.\")  # 调试信息\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    ).to(device).eval()\n",
    "    print(\"Model loaded successfully.\")  # 调试信息\n",
    "    return model, tokenizer\n",
    "\n",
    "# 加载 MMLU-Pro 数据集\n",
    "def load_mmlu_pro():\n",
    "    \"\"\" 加载 MMLU-Pro 数据集 \"\"\"\n",
    "    dataset = load_dataset(\"/root/autodl-tmp/MMLU-PRO\")\n",
    "    test_df, val_df = dataset[\"test\"], dataset[\"validation\"]\n",
    "    return test_df, val_df\n",
    "\n",
    "# 预处理数据\n",
    "def preprocess(test_df):\n",
    "    res_df = []\n",
    "    for each in test_df:\n",
    "        options = [opt for opt in each[\"options\"] if opt != \"N/A\"]\n",
    "        each[\"options\"] = options\n",
    "        res_df.append(each)\n",
    "    return res_df\n",
    "\n",
    "# 筛选 Health 类别的数据\n",
    "def filter_health_category(test_df):\n",
    "    return [item for item in test_df if item[\"category\"] == \"health\"]\n",
    "\n",
    "# 改进输入格式\n",
    "def format_query(question, options):\n",
    "    \"\"\" 格式化问题输入，确保模型能清晰理解 \"\"\"\n",
    "    option_str = \"\\n\".join([f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(options)])\n",
    "    query = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Options:\\n{option_str}\\n\"\n",
    "        f\"Please choose the correct answer from the options (A, B, C, etc.). Answer in this format: Correct answer: \"\n",
    "    )\n",
    "    return query\n",
    "\n",
    "# 生成文本描述（适配 LLaMA）\n",
    "def generate_description(model, tokenizer, query):\n",
    "    \"\"\" 生成文本描述 \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"错误：请输入问题。\"\n",
    "\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 200,  # ✅ 生成的新 token 数量，而不是限制总 token 长度\n",
    "        \"do_sample\": False,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        inference_time = time.time() - start_time\n",
    "        description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return description, inference_time\n",
    "\n",
    "\n",
    "# 提取模型答案（改进版）\n",
    "def extract_answer(text):\n",
    "    \"\"\" 从模型输出中提取答案，支持更多格式 \"\"\"\n",
    "    patterns = [\n",
    "        r\"answer is \\(?([A-Z])\\)?\",  # answer is A\n",
    "        r\"Correct answer: ([A-Z])\",  # Correct answer: A\n",
    "        r\"([A-Z]) is correct\",       # A is correct\n",
    "        r\"\\bOption ([A-Z])\\b\"        # Option A\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "    return None  # 未能匹配到答案\n",
    "\n",
    "# 测试模型\n",
    "def test_model(model, tokenizer, test_df):\n",
    "    results = []\n",
    "    total_time = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for item in tqdm(test_df, desc=\"Testing\"):\n",
    "        question = item[\"question\"]\n",
    "        options = item[\"options\"]\n",
    "        answer = item[\"answer\"]\n",
    "\n",
    "        # 格式化输入\n",
    "        query = format_query(question, options)\n",
    "\n",
    "        # 生成描述\n",
    "        description, inference_time = generate_description(model, tokenizer, query)\n",
    "        total_time += inference_time\n",
    "\n",
    "        # 提取模型答案\n",
    "        model_answer = extract_answer(description)\n",
    "\n",
    "        # 检查是否正确\n",
    "        is_correct = model_answer == answer\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        # 保存结果\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": answer,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"inference_time\": inference_time\n",
    "        })\n",
    "\n",
    "    # 计算准确率和平均推理时间\n",
    "    accuracy = correct / len(test_df)\n",
    "    avg_inference_time = total_time / len(test_df)\n",
    "\n",
    "    return results, accuracy, avg_inference_time\n",
    "\n",
    "# 保存测试结果\n",
    "def save_results(results, output_path):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "# 主函数\n",
    "def main_jupyter(model_path=\"/root/autodl-tmp/trained_model\", output_path=\"/root/autodl-tmp/health_test_results.json\"):\n",
    "    # 加载模型\n",
    "    model, tokenizer = load_model(model_path)\n",
    "\n",
    "    # 加载 MMLU-Pro 数据集\n",
    "    test_df, val_df = load_mmlu_pro()\n",
    "    test_df = preprocess(test_df)\n",
    "\n",
    "    # 筛选 Health 类别的数据\n",
    "    health_test_df = filter_health_category(test_df)\n",
    "    print(f\"Health 类别数据量: {len(health_test_df)}\")\n",
    "\n",
    "    # 测试模型\n",
    "    results, accuracy, avg_inference_time = test_model(model, tokenizer, health_test_df)\n",
    "\n",
    "    # 保存结果\n",
    "    save_results(results, output_path)\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"测试完成！\\nHealth 类别准确率: {accuracy * 100:.2f}%\\n平均推理时间: {avg_inference_time:.4f} 秒\")\n",
    "\n",
    "# 在 Jupyter Notebook 中直接调用\n",
    "main_jupyter()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
