{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193f9313-ab4d-48b3-9ba8-e3e822b7170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /root/autodl-tmp/glm-4v-9b\n",
      "Tokenizer loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc7bb26b7df46fa82ecf5a493bc9c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Loading text embedding model from: /root/autodl-tmp/all-MiniLM-L6-v2\n",
      "Text embedding model loaded successfully.\n",
      "Loading text retrieval system from: /root/autodl-tmp/text_index.faiss and /root/autodl-tmp/text_texts.json\n",
      "Text retrieval system loaded successfully.\n",
      "Health 类别数据量: 818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/818 [00:00<?, ?it/s]/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Testing: 100%|██████████| 818/818 [04:10<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试完成！\n",
      "Health 类别准确率: 35.45%\n",
      "平均推理时间: 0.2601 秒\n",
      "检索成功率: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 设备设置\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载 GLM-4V 模型\n",
    "def load_model(model_path):\n",
    "    print(f\"Loading model from: {model_path}\")  # 调试信息\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    print(\"Tokenizer loaded successfully.\")  # 调试信息\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    ).to(device).eval()\n",
    "    print(\"Model loaded successfully.\")  # 调试信息\n",
    "    return model, tokenizer\n",
    "\n",
    "# 加载文本嵌入模型\n",
    "def load_text_embedding_model(model_path):\n",
    "    print(f\"Loading text embedding model from: {model_path}\")  # 调试信息\n",
    "    model = SentenceTransformer(model_path)\n",
    "    print(\"Text embedding model loaded successfully.\")  # 调试信息\n",
    "    return model\n",
    "\n",
    "# 加载文本检索系统\n",
    "def load_text_retrieval_system(index_file, texts_file):\n",
    "    \"\"\"\n",
    "    加载文本检索系统（FAISS 索引和文本数据）。\n",
    "    \"\"\"\n",
    "    print(f\"Loading text retrieval system from: {index_file} and {texts_file}\")  # 调试信息\n",
    "    index = faiss.read_index(index_file)\n",
    "    with open(texts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = json.load(f)\n",
    "    print(\"Text retrieval system loaded successfully.\")  # 调试信息\n",
    "    return index, texts\n",
    "\n",
    "# 检索文本\n",
    "def retrieve_texts(query, index, texts, k=3):\n",
    "    \"\"\"\n",
    "    检索与查询相关的文本。\n",
    "    \"\"\"\n",
    "    query_embedding = text_embedding_model.encode([query], convert_to_tensor=False)\n",
    "    distances, indices = index.search(query_embedding.astype(\"float32\"), k)\n",
    "    retrieved_texts = [texts[i] for i in indices[0]]\n",
    "    return retrieved_texts\n",
    "\n",
    "# 加载 MMLU-Pro 数据集\n",
    "def load_mmlu_pro():\n",
    "    \"\"\" 加载 MMLU-Pro 数据集 \"\"\"\n",
    "    dataset = load_dataset(\"/root/autodl-tmp/MMLU-Pro\")\n",
    "    test_df, val_df = dataset[\"test\"], dataset[\"validation\"]\n",
    "    return test_df, val_df\n",
    "\n",
    "# 预处理数据\n",
    "def preprocess(test_df):\n",
    "    res_df = []\n",
    "    for each in test_df:\n",
    "        options = [opt for opt in each[\"options\"] if opt != \"N/A\"]\n",
    "        each[\"options\"] = options\n",
    "        res_df.append(each)\n",
    "    return res_df\n",
    "\n",
    "# 筛选 Health 类别的数据\n",
    "def filter_health_category(test_df):\n",
    "    return [item for item in test_df if item[\"category\"] == \"health\"]\n",
    "\n",
    "# 改进输入格式\n",
    "def format_query(question, options):\n",
    "    \"\"\" 格式化问题输入，确保模型能清晰理解 \"\"\"\n",
    "    option_str = \"\\n\".join([f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(options)])\n",
    "    query = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Options:\\n{option_str}\\n\"\n",
    "        f\"Please choose the correct answer from the options (A, B, C, etc.). Answer in this format: Correct answer: \"\n",
    "    )\n",
    "    return query\n",
    "\n",
    "# 生成描述（集成 RAG）\n",
    "def generate_description_with_rag(model, tokenizer, query, index, texts):\n",
    "    \"\"\"\n",
    "    生成描述，集成 RAG 功能。\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"错误：请输入问题。\"\n",
    "\n",
    "    # 检索相关文档\n",
    "    retrieved_texts = retrieve_texts(query, index, texts)\n",
    "    context = \"Retrieved documents:\\n\" + \"\\n\".join(retrieved_texts) + \"\\n\\nQuestion: \" + query\n",
    "\n",
    "    # 处理文本输入\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": context}],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "    ).to(device)\n",
    "\n",
    "    # 生成回答\n",
    "    gen_kwargs = {\"max_length\": 2000, \"do_sample\": False, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        inference_time = time.time() - start_time\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return description, retrieved_texts, inference_time\n",
    "\n",
    "# 提取模型答案（改进版）\n",
    "def extract_answer(text):\n",
    "    \"\"\" 从模型输出中提取答案，支持更多格式 \"\"\"\n",
    "    patterns = [\n",
    "        r\"answer is \\(?([A-Z])\\)?\",  # answer is A\n",
    "        r\"Correct answer: ([A-Z])\",  # Correct answer: A\n",
    "        r\"([A-Z]) is correct\",       # A is correct\n",
    "        r\"\\bOption ([A-Z])\\b\"        # Option A\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "    return None  # 未能匹配到答案\n",
    "\n",
    "# 测试模型（集成 RAG）\n",
    "def test_model_with_rag(model, tokenizer, test_df, index, texts):\n",
    "    results = []\n",
    "    total_time = 0.0\n",
    "    correct = 0\n",
    "    retrieval_success = 0  # 检索成功的次数\n",
    "\n",
    "    for item in tqdm(test_df, desc=\"Testing\"):\n",
    "        question = item[\"question\"]\n",
    "        options = item[\"options\"]\n",
    "        answer = item[\"answer\"]\n",
    "\n",
    "        # 格式化输入\n",
    "        query = format_query(question, options)\n",
    "\n",
    "        # 生成描述（集成 RAG）\n",
    "        description, retrieved_texts, inference_time = generate_description_with_rag(model, tokenizer, query, index, texts)\n",
    "        total_time += inference_time\n",
    "\n",
    "        # 提取模型答案\n",
    "        model_answer = extract_answer(description)\n",
    "\n",
    "        # 检查是否正确\n",
    "        is_correct = model_answer == answer\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        # 检查检索是否成功\n",
    "        if retrieved_texts:  # 如果有检索到的文本，则认为检索成功\n",
    "            retrieval_success += 1\n",
    "\n",
    "        # 保存结果\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": answer,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"inference_time\": inference_time,\n",
    "            \"retrieved_texts\": retrieved_texts\n",
    "        })\n",
    "\n",
    "    # 计算准确率和平均推理时间\n",
    "    accuracy = correct / len(test_df)\n",
    "    avg_inference_time = total_time / len(test_df)\n",
    "    retrieval_success_rate = retrieval_success / len(test_df)\n",
    "\n",
    "    return results, accuracy, avg_inference_time, retrieval_success_rate\n",
    "\n",
    "# 保存测试结果\n",
    "def save_results(results, output_path):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "# 主函数\n",
    "def main_jupyter(model_path=\"/root/autodl-tmp/glm-4v-9b\", output_path=\"/root/autodl-tmp/health_test_results_with_rag.json\"):\n",
    "    # 加载模型\n",
    "    model, tokenizer = load_model(model_path)\n",
    "\n",
    "    # 加载文本嵌入模型\n",
    "    global text_embedding_model\n",
    "    text_embedding_model = load_text_embedding_model(\"/root/autodl-tmp/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 加载文本检索系统\n",
    "    index_file = \"/root/autodl-tmp/text_index.faiss\"\n",
    "    texts_file = \"/root/autodl-tmp/text_texts.json\"\n",
    "    index, texts = load_text_retrieval_system(index_file, texts_file)\n",
    "\n",
    "    # 加载 MMLU-Pro 数据集\n",
    "    test_df, val_df = load_mmlu_pro()\n",
    "    test_df = preprocess(test_df)\n",
    "\n",
    "    # 筛选 Health 类别的数据\n",
    "    health_test_df = filter_health_category(test_df)\n",
    "    print(f\"Health 类别数据量: {len(health_test_df)}\")\n",
    "\n",
    "    # 测试模型（集成 RAG）\n",
    "    results, accuracy, avg_inference_time, retrieval_success_rate = test_model_with_rag(model, tokenizer, health_test_df, index, texts)\n",
    "\n",
    "    # 保存结果\n",
    "    save_results(results, output_path)\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"测试完成！\\nHealth 类别准确率: {accuracy * 100:.2f}%\\n平均推理时间: {avg_inference_time:.4f} 秒\")\n",
    "    print(f\"检索成功率: {retrieval_success_rate * 100:.2f}%\")\n",
    "\n",
    "# 在 Jupyter Notebook 中直接调用\n",
    "main_jupyter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
