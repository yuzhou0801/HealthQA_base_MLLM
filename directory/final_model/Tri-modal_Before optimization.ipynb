{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829c2466-5bab-4c4a-b10e-46343fcaf3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd237994eea4441922164c752d1e5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "Running on public URL: https://4041d8f124070fcd45.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4041d8f124070fcd45.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gradio as gr\n",
    "import whisper  # ç”¨äºè¯­éŸ³è½¬æ–‡æœ¬\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# åŠ è½½ GLM-4V æ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/glm-4v-9b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/root/autodl-tmp/glm-4v-9b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "# åŠ è½½ Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹\n",
    "whisper_model = whisper.load_model(\"base\")  # ç¡®ä¿åŠ è½½æˆåŠŸ\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\" è¯­éŸ³è½¬æ–‡æœ¬ï¼Œå¹¶æä¾›çŠ¶æ€ä¿¡æ¯ \"\"\"\n",
    "    if not audio_path:\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"ğŸ” æ­£åœ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶: {audio_path}\")  # DEBUG: ç¡®ä¿è·¯å¾„æœ‰æ•ˆ\n",
    "\n",
    "    try:\n",
    "        transcription = whisper_model.transcribe(audio_path)\n",
    "        text_output = transcription[\"text\"]\n",
    "        return text_output if text_output.strip() else \"\"\n",
    "    except Exception as e:\n",
    "        return f\"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}\"\n",
    "\n",
    "def generate_description(image, query):\n",
    "    \"\"\" ç”Ÿæˆæ–‡æœ¬æè¿° \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"é”™è¯¯ï¼šè¯·è¾“å…¥æ–‡æœ¬æˆ–è¯­éŸ³è¾“å…¥é—®é¢˜ã€‚\"\n",
    "\n",
    "    # å¤„ç†å›¾ç‰‡ + æ–‡æœ¬è¾“å…¥\n",
    "    if image is not None:\n",
    "        image = image.convert('RGB')\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"image\": image, \"content\": query}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True\n",
    "        ).to(device)\n",
    "    else:\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": query}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True\n",
    "        ).to(device)\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 1000, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return description\n",
    "\n",
    "def update_query_from_audio(audio):\n",
    "    \"\"\" è¯­éŸ³è¾“å…¥åï¼Œè‡ªåŠ¨å¡«å…… `è¯­éŸ³è½¬æ–‡æœ¬` è¾“å…¥æ¡† \"\"\"\n",
    "    transcribed_text = transcribe_audio(audio)\n",
    "    return transcribed_text  # è®©è¯­éŸ³è½¬æ–‡å­—ç›´æ¥å¡«å…… UI è¾“å…¥æ¡†\n",
    "\n",
    "def gradio_interface(image, transcribed_text, query):\n",
    "    \"\"\" å¤„ç†è¾“å…¥ï¼Œå¹¶ç¡®ä¿ query ç”± `è¯­éŸ³è½¬æ–‡æœ¬` + `æ‰‹åŠ¨è¾“å…¥` å…±åŒå†³å®š \"\"\"\n",
    "    final_query = query.strip() or transcribed_text.strip()  # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬\n",
    "\n",
    "    if not final_query:\n",
    "        return \"é”™è¯¯ï¼šè¯·è¾“å…¥æ–‡æœ¬æˆ–è¯­éŸ³è¾“å…¥é—®é¢˜ã€‚\"\n",
    "\n",
    "    description = generate_description(image, final_query)\n",
    "    return description\n",
    "\n",
    "# Gradio ç•Œé¢\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## GLM-4V è¯­éŸ³ + å›¾ç‰‡ + æ–‡æœ¬ å¤šæ¨¡æ€æè¿°ç”Ÿæˆ\")\n",
    "    gr.Markdown(\"ä¸Šä¼ å›¾ç‰‡ã€è¾“å…¥é—®é¢˜æˆ–ä½¿ç”¨è¯­éŸ³æè¿°ï¼Œè®© AI ç”Ÿæˆå¯¹åº”çš„æè¿°ã€‚\")\n",
    "\n",
    "    with gr.Row():\n",
    "        image_input = gr.Image(label=\"ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰\", type=\"pil\")\n",
    "\n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"è¯­éŸ³è¾“å…¥ï¼ˆå¯é€‰ï¼‰\")\n",
    "        transcribed_text = gr.Textbox(label=\"è¯­éŸ³è½¬æ–‡æœ¬ç»“æœï¼ˆå¯ä¿®æ”¹ï¼‰\", interactive=True)  # è¯­éŸ³è½¬æ–‡æœ¬å®æ—¶å¡«å……\n",
    "\n",
    "    with gr.Row():\n",
    "        query_input = gr.Textbox(label=\"è¾“å…¥é—®é¢˜ï¼ˆå¯æ‰‹åŠ¨ä¿®æ”¹ï¼‰\", interactive=True)  # æœ€ç»ˆè¾“å…¥\n",
    "        submit_button = gr.Button(\"æäº¤\")\n",
    "\n",
    "    output_text = gr.Textbox(label=\"ç”Ÿæˆçš„æè¿°\")\n",
    "\n",
    "    # å½“ç”¨æˆ·ä¸Šä¼ éŸ³é¢‘æ—¶ï¼Œæ›´æ–° `è¯­éŸ³è½¬æ–‡æœ¬` è¾“å…¥æ¡†\n",
    "    audio_input.change(update_query_from_audio, inputs=[audio_input], outputs=[transcribed_text])\n",
    "\n",
    "    # æäº¤æ—¶ï¼Œç»¼åˆ `è¯­éŸ³è½¬æ–‡æœ¬` + `æ‰‹åŠ¨è¾“å…¥`ï¼Œç”Ÿæˆæè¿°\n",
    "    submit_button.click(\n",
    "        gradio_interface,\n",
    "        inputs=[image_input, transcribed_text, query_input],\n",
    "        outputs=[output_text]\n",
    "    )\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
