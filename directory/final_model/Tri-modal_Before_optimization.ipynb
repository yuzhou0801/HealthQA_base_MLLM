{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829c2466-5bab-4c4a-b10e-46343fcaf3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd237994eea4441922164c752d1e5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "Running on public URL: https://4041d8f124070fcd45.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4041d8f124070fcd45.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gradio as gr\n",
    "import whisper  # 用于语音转文本\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载 GLM-4V 模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/glm-4v-9b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/root/autodl-tmp/glm-4v-9b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "# 加载 Whisper 语音识别模型\n",
    "whisper_model = whisper.load_model(\"base\")  # 确保加载成功\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\" 语音转文本，并提供状态信息 \"\"\"\n",
    "    if not audio_path:\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"🔍 正在处理音频文件: {audio_path}\")  # DEBUG: 确保路径有效\n",
    "\n",
    "    try:\n",
    "        transcription = whisper_model.transcribe(audio_path)\n",
    "        text_output = transcription[\"text\"]\n",
    "        return text_output if text_output.strip() else \"\"\n",
    "    except Exception as e:\n",
    "        return f\"语音识别失败: {str(e)}\"\n",
    "\n",
    "def generate_description(image, query):\n",
    "    \"\"\" 生成文本描述 \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"错误：请输入文本或语音输入问题。\"\n",
    "\n",
    "    # 处理图片 + 文本输入\n",
    "    if image is not None:\n",
    "        image = image.convert('RGB')\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"image\": image, \"content\": query}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True\n",
    "        ).to(device)\n",
    "    else:\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": query}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True\n",
    "        ).to(device)\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 1000, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return description\n",
    "\n",
    "def update_query_from_audio(audio):\n",
    "    \"\"\" 语音输入后，自动填充 `语音转文本` 输入框 \"\"\"\n",
    "    transcribed_text = transcribe_audio(audio)\n",
    "    return transcribed_text  # 让语音转文字直接填充 UI 输入框\n",
    "\n",
    "def gradio_interface(image, transcribed_text, query):\n",
    "    \"\"\" 处理输入，并确保 query 由 `语音转文本` + `手动输入` 共同决定 \"\"\"\n",
    "    final_query = query.strip() or transcribed_text.strip()  # 优先使用用户输入的文本\n",
    "\n",
    "    if not final_query:\n",
    "        return \"错误：请输入文本或语音输入问题。\"\n",
    "\n",
    "    description = generate_description(image, final_query)\n",
    "    return description\n",
    "\n",
    "# Gradio 界面\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## GLM-4V 语音 + 图片 + 文本 多模态描述生成\")\n",
    "    gr.Markdown(\"上传图片、输入问题或使用语音描述，让 AI 生成对应的描述。\")\n",
    "\n",
    "    with gr.Row():\n",
    "        image_input = gr.Image(label=\"上传图片（可选）\", type=\"pil\")\n",
    "\n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"语音输入（可选）\")\n",
    "        transcribed_text = gr.Textbox(label=\"语音转文本结果（可修改）\", interactive=True)  # 语音转文本实时填充\n",
    "\n",
    "    with gr.Row():\n",
    "        query_input = gr.Textbox(label=\"输入问题（可手动修改）\", interactive=True)  # 最终输入\n",
    "        submit_button = gr.Button(\"提交\")\n",
    "\n",
    "    output_text = gr.Textbox(label=\"生成的描述\")\n",
    "\n",
    "    # 当用户上传音频时，更新 `语音转文本` 输入框\n",
    "    audio_input.change(update_query_from_audio, inputs=[audio_input], outputs=[transcribed_text])\n",
    "\n",
    "    # 提交时，综合 `语音转文本` + `手动输入`，生成描述\n",
    "    submit_button.click(\n",
    "        gradio_interface,\n",
    "        inputs=[image_input, transcribed_text, query_input],\n",
    "        outputs=[output_text]\n",
    "    )\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
