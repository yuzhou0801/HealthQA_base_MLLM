{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630062ca-daf0-44e7-b885-00fd5badab1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a58ffc5cad0474284141e66e596f38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://3e901690f3d64d5b42.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3e901690f3d64d5b42.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gradio as gr\n",
    "import whisper\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# 设备设置\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载 GLM-4V 模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/glm-4v-9b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/root/autodl-tmp/glm-4v-9b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "# 加载 Whisper 语音识别模型\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "# 加载文本嵌入模型\n",
    "text_embedding_model = SentenceTransformer(\"/root/autodl-tmp/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 加载 CLIP 模型和处理器\n",
    "clip_model = CLIPModel.from_pretrained(\"/root/autodl-tmp/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"/root/autodl-tmp/clip-vit-base-patch32\")\n",
    "\n",
    "# 加载文本检索系统\n",
    "def load_text_retrieval_system(index_file, texts_file):\n",
    "    \"\"\"\n",
    "    加载文本检索系统（FAISS 索引和文本数据）。\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(index_file)\n",
    "    with open(texts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = json.load(f)\n",
    "    return index, texts\n",
    "\n",
    "# 加载多模态检索系统\n",
    "def load_multimodal_retrieval_system(index_file, texts_file):\n",
    "    \"\"\"\n",
    "    加载多模态检索系统（FAISS 索引和图片文本数据）。\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(index_file)\n",
    "    with open(texts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        image_texts = json.load(f)\n",
    "    return index, image_texts\n",
    "\n",
    "# 检索文本（支持多个数据库）\n",
    "def retrieve_texts(query, indices, texts_list, k=3):\n",
    "    \"\"\"\n",
    "    检索与查询相关的文本，支持多个数据库。\n",
    "    \"\"\"\n",
    "    all_retrieved_texts = []\n",
    "    for index, texts in zip(indices, texts_list):\n",
    "        query_embedding = text_embedding_model.encode([query], convert_to_tensor=False)\n",
    "        distances, indices = index.search(query_embedding.astype(\"float32\"), k)\n",
    "        retrieved_texts = [texts[i] for i in indices[0]]\n",
    "        all_retrieved_texts.extend(retrieved_texts)\n",
    "    return all_retrieved_texts\n",
    "\n",
    "# 分块处理长文本\n",
    "def chunk_text(text, max_length=77):\n",
    "    \"\"\"\n",
    "    将长文本分割成多个短文本块，每个块的长度不超过 max_length。\n",
    "    \"\"\"\n",
    "    words = text.split()  # 按空格分割文本为单词列表\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        # 如果当前块加上新单词后长度不超过 max_length，则添加到当前块\n",
    "        if len(\" \".join(current_chunk + [word])) <= max_length:\n",
    "            current_chunk.append(word)\n",
    "        else:\n",
    "            # 否则，将当前块保存，并开始一个新块\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    # 添加最后一个块\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# 检索多模态数据（支持分块处理）\n",
    "def retrieve_multimodal_data(query, index, image_texts, k=3):\n",
    "    \"\"\"\n",
    "    检索与查询相关的多模态数据（图片和文本）。\n",
    "    \"\"\"\n",
    "    # 分块处理长文本\n",
    "    chunks = chunk_text(query)\n",
    "\n",
    "    # 对每个文本块获取嵌入\n",
    "    chunk_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        inputs = clip_processor(text=chunk, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            chunk_embedding = clip_model.get_text_features(**inputs).cpu().numpy()\n",
    "        chunk_embeddings.append(chunk_embedding)\n",
    "\n",
    "    # 聚合所有文本块的嵌入（取平均值）\n",
    "    if chunk_embeddings:\n",
    "        query_text_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "    else:\n",
    "        query_text_embedding = np.zeros((1, 512))  # 如果没有文本块，返回零向量\n",
    "\n",
    "    # 构建查询嵌入\n",
    "    query_embedding = np.concatenate([np.zeros((1, 512)), query_text_embedding], axis=1)  # 图片部分用零填充\n",
    "    distances, indices = index.search(query_embedding.astype(\"float32\"), k)\n",
    "    retrieved_data = [image_texts[i] for i in indices[0]]\n",
    "    return retrieved_data\n",
    "\n",
    "# 语音转文本\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"\n",
    "    将语音转换为文本。\n",
    "    \"\"\"\n",
    "    if not audio_path:\n",
    "        return \"\"\n",
    "    try:\n",
    "        transcription = whisper_model.transcribe(audio_path)\n",
    "        return transcription[\"text\"]\n",
    "    except Exception as e:\n",
    "        return f\"语音识别失败: {str(e)}\"\n",
    "\n",
    "# 生成描述（集成 RAG）\n",
    "def generate_description(image, query, text_indices, text_texts_list, image_index, image_texts):\n",
    "    \"\"\"\n",
    "    生成描述，集成 RAG 功能。\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"错误：请输入文本或语音输入问题。\"\n",
    "\n",
    "    # 检索相关文档（支持多个数据库）\n",
    "    retrieved_texts = retrieve_texts(query, text_indices, text_texts_list)\n",
    "    retrieved_images = retrieve_multimodal_data(query, image_index, image_texts)\n",
    "\n",
    "    # 将检索结果与用户输入结合\n",
    "    context = \"检索到的文本信息：\\n\" + \"\\n\".join(retrieved_texts) + \"\\n\\n检索到的图片信息：\\n\" + \"\\n\".join(\n",
    "        [item[\"text\"] for item in retrieved_images]\n",
    "    )\n",
    "    final_query = f\"{context}\\n\\n用户问题：{query}\"\n",
    "\n",
    "    # 处理图片 + 文本输入\n",
    "    if image is not None:\n",
    "        image = image.convert('RGB')\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"image\": image, \"content\": final_query}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True\n",
    "        ).to(device)\n",
    "    else:\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": final_query}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True\n",
    "        ).to(device)\n",
    "\n",
    "    # 生成回答\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 1000,  # 明确指定生成的最大 token 数量\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return description\n",
    "\n",
    "# 更新查询（语音转文本）\n",
    "def update_query_from_audio(audio):\n",
    "    \"\"\"\n",
    "    更新查询内容（语音转文本）。\n",
    "    \"\"\"\n",
    "    return transcribe_audio(audio)\n",
    "\n",
    "# Gradio 界面\n",
    "def gradio_interface(image, transcribed_text, query):\n",
    "    \"\"\"\n",
    "    Gradio 界面逻辑。\n",
    "    \"\"\"\n",
    "    final_query = query.strip() or transcribed_text.strip()\n",
    "    if not final_query:\n",
    "        return \"Error: Please enter text or voice input problem.\"\n",
    "\n",
    "    description = generate_description(image, final_query, text_indices, text_texts_list, image_index, image_texts)\n",
    "    return description\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    global text_indices, text_texts_list, image_index, image_texts\n",
    "\n",
    "    # 加载检索系统\n",
    "    text_index_file_1 = \"/root/autodl-tmp/text_index.faiss\"  # 原有文本 FAISS 索引文件路径\n",
    "    text_texts_file_1 = \"/root/autodl-tmp/text_texts.json\"   # 原有文本数据文件路径\n",
    "    text_index_file_2 = \"/root/autodl-tmp/NHS_text_index.faiss\"  # 新增文本 FAISS 索引文件路径\n",
    "    text_texts_file_2 = \"/root/autodl-tmp/NHS_text_texts.json\"   # 新增文本数据文件路径\n",
    "    image_index_file = \"/root/autodl-tmp/image_index.faiss\"    # 图片 FAISS 索引文件路径\n",
    "    image_texts_file = \"/root/autodl-tmp/image_texts.json\"     # 图片文本数据文件路径\n",
    "\n",
    "    # 加载多个文本数据库\n",
    "    text_index_1, text_texts_1 = load_text_retrieval_system(text_index_file_1, text_texts_file_1)\n",
    "    text_index_2, text_texts_2 = load_text_retrieval_system(text_index_file_2, text_texts_file_2)\n",
    "    text_indices = [text_index_1, text_index_2]\n",
    "    text_texts_list = [text_texts_1, text_texts_2]\n",
    "\n",
    "    # 加载多模态数据库\n",
    "    image_index, image_texts = load_multimodal_retrieval_system(image_index_file, image_texts_file)\n",
    "\n",
    "    # Gradio 界面\n",
    "    with gr.Blocks() as interface:\n",
    "        gr.Markdown(\"## GLM-4V Voice + Picture + Text Multimodal Description Generation (Integrated with RAG)\")\n",
    "        gr.Markdown(\"Upload a picture, enter a question, or use voice description to let AI generate the corresponding description.\")\n",
    "\n",
    "        with gr.Row():\n",
    "            image_input = gr.Image(label=\"Upload a picture (optional)\", type=\"pil\")\n",
    "\n",
    "        with gr.Row():\n",
    "            audio_input = gr.Audio(type=\"filepath\", label=\"Voice input (optional)\")\n",
    "            transcribed_text = gr.Textbox(label=\"Speech-to-text results (editable)\", interactive=True)\n",
    "\n",
    "        with gr.Row():\n",
    "            query_input = gr.Textbox(label=\"Input question (can be edited manually)\", interactive=True)\n",
    "            submit_button = gr.Button(\"submit\")\n",
    "\n",
    "        output_text = gr.Textbox(label=\"Generated Description\")\n",
    "\n",
    "        # 当用户上传音频时，更新 语音转文本 输入框\n",
    "        audio_input.change(update_query_from_audio, inputs=[audio_input], outputs=[transcribed_text])\n",
    "\n",
    "        # 提交时，综合 语音转文本 + 手动输入，生成描述\n",
    "        submit_button.click(\n",
    "            gradio_interface,\n",
    "            inputs=[image_input, transcribed_text, query_input],\n",
    "            outputs=[output_text]\n",
    "        )\n",
    "\n",
    "    interface.launch(share=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
