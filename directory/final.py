import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import faiss
from sentence_transformers import SentenceTransformer
import gradio as gr
import whisper
from gtts import gTTS
import tempfile

# å…¨å±€å˜é‡ï¼ˆé¿å…é€šè¿‡ gr.State ä¼ é€’å¤§å‹å¯¹è±¡ï¼‰
model = None
tokenizer = None
index = None
texts = None
whisper_model = None

# è®¾å¤‡è®¾ç½®
device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½æ–‡æœ¬åµŒå…¥æ¨¡å‹
text_embedding_model = SentenceTransformer(r"D:\all-MiniLM-L6-v2")


# åŠ è½½æ–‡æœ¬æ£€ç´¢ç³»ç»Ÿ
def load_text_retrieval_system(index_file, texts_file):
    """
    åŠ è½½æ–‡æœ¬æ£€ç´¢ç³»ç»Ÿï¼ˆFAISS ç´¢å¼•å’Œæ–‡æœ¬æ•°æ®ï¼‰ã€‚
    """
    index = faiss.read_index(index_file)
    with open(texts_file, "r", encoding="utf-8") as f:
        texts = json.load(f)
    return index, texts


# æ£€ç´¢æ–‡æœ¬
def retrieve_texts(query, index, texts, k=3, threshold=0.6):
    """ æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æœ¬ï¼Œå¹¶è®¾å®šç›¸ä¼¼åº¦é˜ˆå€¼ """
    query_embedding = text_embedding_model.encode([query], convert_to_tensor=False)
    distances, indices = index.search(query_embedding.astype("float32"), k)
    retrieved_texts = []
    for i, dist in zip(indices[0], distances[0]):
        if dist < threshold:  # è®¾å®šç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé¿å…ä½ç›¸å…³æ€§æ–‡æœ¬
            retrieved_texts.append(texts[i])
    return retrieved_texts


# åŠ è½½å¾®è°ƒå¥½çš„æ¨¡å‹
def load_model(model_path):
    print(f"Loading model from: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        trust_remote_code=True
    ).to(device).eval()
    return model, tokenizer


# åŠ è½½ Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹
def load_whisper_model():
    global whisper_model
    print("Loading Whisper model...")
    whisper_model = whisper.load_model("base")


# è¯­éŸ³è½¬æ–‡å­—å‡½æ•°
def transcribe_audio(audio_file):
    if audio_file is None:
        return ""
    # ä½¿ç”¨ Whisper å¯¹å½•éŸ³æ–‡ä»¶è¿›è¡Œè½¬å½•
    result = whisper_model.transcribe(audio_file)
    text = result["text"].strip()
    return text


# æ–‡æœ¬è½¬è¯­éŸ³å‡½æ•°ï¼Œç”Ÿæˆä¸´æ—¶ mp3 æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„
def text_to_speech(text):
    if not text:
        return None
    tts = gTTS(text, lang="en")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        tts.write_to_fp(fp)
        temp_filename = fp.name
    return temp_filename


# æå–æœ€åä¸€æ¡åŠ©æ‰‹å›å¤å¹¶è½¬æ¢ä¸ºè¯­éŸ³
def play_answer(history):
    # history ä¸º openai-style æ ¼å¼çš„å¯¹è¯è®°å½•åˆ—è¡¨
    answer = None
    for msg in reversed(history):
        if msg.get("role") == "assistant" and msg.get("content"):
            answer = msg["content"]
            break
    if not answer:
        return None
    audio_file = text_to_speech(answer)
    return audio_file


# åˆ‡æ¢è¯­éŸ³æ’­æŠ¥ï¼šç‚¹å‡»åè‡ªåŠ¨ç”Ÿæˆå¹¶æ’­æ”¾ï¼Œé‡å¤ç‚¹å‡»åˆ™åœæ­¢æ’­æ”¾
def toggle_tts(chat_history, is_playing):
    if not is_playing:
        audio_file = play_answer(chat_history)
        if not audio_file:
            # æ²¡æœ‰ç”Ÿæˆè¯­éŸ³åˆ™ä¿æŒæœªæ’­æ”¾çŠ¶æ€
            return gr.update(value=None), "Voice broadcast", False
        # è¿”å›æ›´æ–°åçš„éšè—éŸ³é¢‘ç»„ä»¶ï¼ˆè®¾ç½® autoplay=Trueï¼‰ã€æŒ‰é’®æ–‡æœ¬æ›´æ–°ä¸ºâ€œåœæ­¢æ’­æ”¾â€åŠæ’­æ”¾çŠ¶æ€ True
        return gr.update(value=audio_file, autoplay=True), "Stop Playing", True
    else:
        # åœæ­¢æ’­æ”¾ï¼šæ¸…ç©ºéŸ³é¢‘ç»„ä»¶ï¼Œå¹¶å°†æŒ‰é’®æ–‡æœ¬æ¢å¤ä¸ºâ€œè¯­éŸ³æ’­æŠ¥â€
        return gr.update(value=None), "Voice broadcast", False


# æ ¼å¼åŒ–ç”¨æˆ·è¾“å…¥
def format_query(user_input, retrieved_texts):
    """ æ ¼å¼åŒ–ç”¨æˆ·è¾“å…¥ï¼Œå¹¶ç¡®ä¿æ£€ç´¢æ–‡æœ¬ä¸ä¼šå¹²æ‰°æ¨¡å‹ """
    if retrieved_texts:
        context = "\n\n".join(f"Retrieved Document {i + 1}: {text}" for i, text in enumerate(retrieved_texts))
        formatted_query = (
            f"### Retrieved Context ###\n"
            f"{context}\n\n"
            f"### User Question ###\n"
            f"{user_input}\n\n"
            f"### Assistant Answer ###\n"
        )
    else:
        formatted_query = (
            f"### User Question ###\n"
            f"{user_input}\n\n"
            f"### Assistant Answer ###\n"
        )
    return formatted_query


# ç”Ÿæˆæ¨¡å‹å›å¤ï¼Œæ–°å¢ use_rag å‚æ•°æ§åˆ¶æ˜¯å¦è°ƒç”¨æ£€ç´¢æ¨¡å—
def generate_response_with_rag(user_input, history, use_rag):
    global model, tokenizer, index, texts
    if use_rag:
        retrieved_texts = retrieve_texts(user_input, index, texts, k=3)
    else:
        retrieved_texts = []
    query = format_query(user_input, retrieved_texts)
    response = generate_response(model, tokenizer, query)
    # è®°å½•å¯¹è¯ï¼ˆopenai-style æ ¼å¼ï¼‰
    history.append({"role": "user", "content": user_input})
    history.append({"role": "assistant", "content": response})
    return history


# ç”Ÿæˆæ¨¡å‹å›å¤ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼Œå†…éƒ¨è°ƒç”¨ï¼‰
def generate_response(model, tokenizer, query):
    inputs = tokenizer(query, return_tensors="pt").to(device)
    gen_kwargs = {
        "max_new_tokens": 50,
        "do_sample": True,
        "temperature": 0.7,
        "top_k": 50
    }
    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.replace(query, "").strip()
    return response


# Web UIï¼Œå¸ƒå±€ç±»ä¼¼ ChatGPTï¼ŒåŒæ—¶å¢åŠ è¯­éŸ³è¾“å…¥ã€æ–‡æœ¬è½¬è¯­éŸ³åŠæ’­æ”¾åˆ‡æ¢åŠŸèƒ½
def launch_web_ui():
    with gr.Blocks() as demo:
        gr.Markdown("# ğŸ¤– HealthQA Chatbot")
        # æ–°å¢å¼€å…³ï¼šè°ƒç”¨RAGå¼€å…³ï¼Œé»˜è®¤å¼€å¯
        rag_switch = gr.Checkbox(label="Calling RAG", value=True)
        # èŠå¤©è®°å½•
        chatbot = gr.Chatbot(elem_id="chatbot", type="messages")
        # è¯­éŸ³è¾“å…¥ç»„ä»¶ï¼ˆæ”¯æŒå½•éŸ³å’Œæ–‡ä»¶ä¸Šä¼ ï¼‰
        audio_input = gr.Audio(type="filepath", label="Voice Input")
        # æ–‡æœ¬è¾“å…¥æ¡†å’Œå‘é€æŒ‰é’®å‚ç›´æ’åˆ—
        with gr.Column():
            user_input = gr.Textbox(placeholder="Please enter your question...")
            send_btn = gr.Button("Send")
        # â€œè¯­éŸ³æ’­æŠ¥â€æŒ‰é’®ï¼ˆä¸å†æ˜¾ç¤ºéŸ³é¢‘æ’­æ”¾ç»„ä»¶ï¼‰
        tts_btn = gr.Button("Voice broadcast")
        # éšè—çš„éŸ³é¢‘ç»„ä»¶ç”¨äºè‡ªåŠ¨æ’­æ”¾ï¼ˆä¸ä¼šæ˜¾ç¤ºåœ¨UIä¸­ï¼‰
        tts_audio = gr.Audio(type="filepath", visible=False)
        # éšè—çŠ¶æ€ï¼Œç”¨äºè®°å½•è¯­éŸ³æ˜¯å¦æ­£åœ¨æ’­æ”¾ï¼Œåˆå§‹å€¼ä¸º False
        play_state = gr.State(False)

        # å½•éŸ³å®Œæˆåè‡ªåŠ¨è½¬å†™å¹¶å¡«å…¥æ–‡æœ¬è¾“å…¥æ¡†
        audio_input.change(fn=transcribe_audio, inputs=audio_input, outputs=user_input)
        # å‘é€æŒ‰é’®å’Œå›è½¦æäº¤å‡è°ƒç”¨ç”Ÿæˆå›å¤å‡½æ•°ï¼Œä¼ å…¥æ˜¯å¦è°ƒç”¨ RAG çš„å¼€å…³å€¼
        send_btn.click(fn=generate_response_with_rag, inputs=[user_input, chatbot, rag_switch], outputs=chatbot)
        user_input.submit(fn=generate_response_with_rag, inputs=[user_input, chatbot, rag_switch], outputs=chatbot)
        # ç‚¹å‡»â€œè¯­éŸ³æ’­æŠ¥â€æŒ‰é’®æ—¶ï¼Œæ›´æ–°éšè—éŸ³é¢‘ç»„ä»¶ã€æŒ‰é’®æ–‡æœ¬å’Œæ’­æ”¾çŠ¶æ€
        tts_btn.click(fn=toggle_tts, inputs=[chatbot, play_state], outputs=[tts_audio, tts_btn, play_state])
    demo.launch(share=True)


# ä¸»å‡½æ•°
def main(model_path=r"C:\Users\zy\Downloads\trained_model_MedQA",
         index_file=r"C:\Users\zy\Downloads\rag\NHS_text_index.faiss",
         texts_file=r"C:\Users\zy\Downloads\rag\NHS_text_texts.json"):
    global model, tokenizer, index, texts
    model, tokenizer = load_model(model_path)
    index, texts = load_text_retrieval_system(index_file, texts_file)
    load_whisper_model()
    launch_web_ui()


# è¿è¡Œä¸»ç¨‹åº
if __name__ == "__main__":
    main()
