# HealthQA_base_MLLM
>This project is a senior course project of Ocean University of China and Heriot-Watt University. It aims to optimize the multimodal large language model in health and support tri-modal input of text, image and voice.

## directory
- [Project Introduction](#Introduction)
- [Dataset preparation](#Dataset-preparation)
- [MLLM model preparation](#MLLM-model-preparation)
- [Experimental Results](#Experimental-Results)
- [References](#References)

---

## 🔥 Introduction
The core goals of this project are:
- 🌟 **Support multimodal input**: Combine **text, image, and voice** to improve medical Q&A results.
- 🚀 **Optimize based on LLaVA-Med**: Train and deploy through **AutoDL 4090 GPU**.
- 🔍 **Dataset**: Contains **text Q&A, image Q&A, and textbooks**, used to build the **RAG database**.

## 📂 Dataset preparation
### 📌 Data Source
- **NHS dataset** 📜: Contains **Disease, Symptoms and Treatments** [Visit the NHS website]([链接地址](https://www.nhsinform.scot/illnesses-and-conditions/a-to-z/))
- **Text quiz answers and textbooks** 📜: Contains **type, question and answer** or **type and text**

## 🏗 MLLM model preparation


## 📊 Experimental Results


## 📚 References
