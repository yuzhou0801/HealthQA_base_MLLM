# HealthQA_base_MLLM
>This project is a senior course project of Ocean University of China and Heriot-Watt University. It aims to optimize the multimodal large language model in health and support tri-modal input of text, image and voice.

## directory
- [Project Introduction](#Introduction)
- [Dataset preparation](#Dataset-preparation)
- [MLLM model preparation](#MLLM-model-preparation)
- [Experimental Results](#Experimental-Results)
- [References](#References)

---

## ğŸ”¥ Introduction
The core goals of this project are:
- ğŸŒŸ **Support multimodal input**: Combine **text, image, and voice** to improve medical Q&A results.
- ğŸš€ **Optimize based on LLaVA-Med**: Train and deploy through **AutoDL 4090 GPU**.
- ğŸ” **Dataset**: Contains **text Q&A, image Q&A, and textbooks**, used to build the **RAG database**.

## ğŸ“‚ Dataset preparation
### ğŸ“Œ Data Source
- **NHS dataset** ğŸ“œ: Contains **Disease, Symptoms and Treatments** [Visit the NHS website]([é“¾æ¥åœ°å€](https://www.nhsinform.scot/illnesses-and-conditions/a-to-z/))
- **Text quiz answers and textbooks** ğŸ“œ: Contains **type, question and answer** or **type and text**

## ğŸ— MLLM model preparation


## ğŸ“Š Experimental Results


## ğŸ“š References
